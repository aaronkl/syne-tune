{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Architecture Search for Large Language Models\n",
    "\n",
    "\n",
    "This notebook shows how to use neural architecture search (NAS) to compress a large language model fine-tuned on some target task.\n",
    "The goal is to reduce model size while retaining performance as much as possible. For that, we search for parts of the network (sub-networks), that jointly optimize parameter count and validation error.\n",
    "There are different ways to define sub-networks. Here, we consider subsets of multi-head attention and fully-connected layers.\n",
    "\n",
    "More specifically, our NAS approach consists of two steps:\n",
    "- We first fine-tune the pre-trained model on the target task, by weight-sharing based NAS training strategies. The idea is to treat the pre-trained model as a 'super-network' that contains a large, but finite set of sub-networks. To avoid that sub-networks co-adapt we modify the fine-tuning the super-network by only updated parts of the network (i.e subset of all layers) in each iteration.\n",
    "- In the second step, we use multi-objective search to find a set of sub-networks that optimally trade-off between parameter count and validation error on the target task.\n",
    "\n",
    "At the end we can plot the so-called Pareto set of architecture and select the final model that gives us the right trade-off between model size and validation error.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "- So far we only support the BERT and GPT2 model family\n",
    "- For now we use Syne Tune for the multi-objective search. In the future we will use AMT, which allows us to distribute the search across multiple SageMaker training jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Install requirements\n",
    "\n",
    "Before we get started, we have to install all requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from training import train_supernetwork\n",
    "from parse_model import get_final_model\n",
    "from estimate_efficency import compute_latency\n",
    "import sagemaker\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter\n",
    "from syne_tune.backend.sagemaker_backend.sagemaker_utils import get_execution_role\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We define different hyperparameters for our model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "task_name = 'rte'\n",
    "model_type = 'bert-base-cased'\n",
    "output_dir = 'nas_output_dir'\n",
    "max_seq_length = 128\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Dataset and Evaluation Metric\n",
    "\n",
    "We load the RTE dataset from the GLUE benchmarking suite via the dataset library from HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    \"glue\", task_name\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This follows the standard Huggingface code from this example: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "if model_type.startswith(\"gpt2\"):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing the raw_datasets\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(\n",
    "        *args, padding=padding, max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split Dataset in Training / Validation\n",
    "\n",
    "We need an extra validation set to evaluate different sub-networks during the multi-objective search. We split the training dataset from GLUE into a extra training / validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\n",
    "    \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n",
    "]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"idx\"])\n",
    "test_dataset = test_dataset.remove_columns([\"idx\"])\n",
    "\n",
    "split = train_dataset.train_test_split(\n",
    "    train_size=0.7, seed=0\n",
    ")  # fix seed to make results reproducible\n",
    "valid_dataset = split[\"test\"]\n",
    "\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_type,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_type,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "if model_type.startswith(\"gpt2\"):\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train weight-sharing based Super-Network\n",
    "\n",
    "We now fine-tune our pre-training network, i.e super-network. To control the training we can use the TrainingArguments of the HuggingFace transformer library. We can pass additional arguments to specify if our dataset is regression or not (determines the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=output_dir)\n",
    "training_args.use_accelerate = False # set this to True to distribute training on multiple GPUs\n",
    "training_args.is_regression = False  # set this to True if your dataset is a regression dataset, for example STSB\n",
    "training_args.save_strategy = \"epoch\"\n",
    "training_args.log_dir = '.log_dir'\n",
    "training_args.seed = seed\n",
    "train_supernetwork(model, train_dataloader, eval_dataloader, metric, training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-objective search for sub-networks\n",
    "\n",
    "After fine-tuning, we run multi-objective search by sampling random sub-networks. Compared to single-objective optimization, in the multi-objective setting we usually do not have a single solution that jointly optimizes all objectives. Instead, we collect a set of solution that *dominate* all other solutions at least in one objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import glob\n",
    "from sagemaker.session import Session\n",
    "from typing import Optional\n",
    "import subprocess\n",
    "\n",
    "def upload_to_s3(src_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    session = Session()\n",
    "    s3_bucket = session.default_bucket()\n",
    "    s3_bucket_prefix = \"nas_amt/model_checkpoint\"\n",
    "\n",
    "    s3_path = f's3://{s3_bucket} / {s3_bucket_prefix}'\n",
    "\n",
    "    cmd = f\"aws s3 sync {src_dir} {s3_path}\"\n",
    "    logger.info(f\"upload results to {s3_path} with command {cmd}\")\n",
    "    subprocess.run(cmd.split(\" \"))\n",
    "\n",
    "    return s3_path\n",
    "\n",
    "s3_path = upload_to_s3(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "entry_point = 'evaluate_subnetwork.py'\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "\n",
    "}\n",
    "\n",
    "max_jobs = 5\n",
    "metric_definitions = [{'Name': 'num-parameters', 'Regex': 'number of parameters: ([0-9\\\\.]+)'},\n",
    "                                                   {'Name': 'validation-performance', 'Regex': 'validation score: ([0-9\\\\.]+)'}\n",
    "                                                   ]\n",
    "estimator = HuggingFace(\n",
    "    entry_point=entry_point,\n",
    "    source_dir=str(Path(__file__).parent),\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    instance_count=1,\n",
    "    py_version=\"py39\",\n",
    "    framework_version=\"1.13\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    transformers_version=\"4.26\",\n",
    "    max_run=3600 * 72,\n",
    "    role=get_execution_role(),\n",
    "    volume_size=125,\n",
    "    model_uri = s3_path,\n",
    "    hyperparameters=hyperparameters)\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "tuning_job_name = f\"nas-search-{current_time}\"\n",
    "my_tuner = HyperparameterTuner(estimator=estimator,  # previously-configured Estimator object\n",
    "                               objective_metric_name='validation-performance',\n",
    "                               hyperparameter_ranges={'num_layers': IntegerParameter(0, 12),\n",
    "                                                      'num_heads': IntegerParameter(0, 12),\n",
    "                                                      'num_units': IntegerParameter(0, 3072),\n",
    "                                                      },\n",
    "                               metric_definitions=metric_definitions,\n",
    "                               max_jobs=max_jobs,\n",
    "                               max_parallel_jobs=1)\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "my_tuner.fit(job_name=tuning_job_name)\n",
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "print(tuner_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's plot the Pareto set - the set of final sub-networks - and compare it against the un-pruned network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = my_tuner.analytics().dataframe()\n",
    "data = np.empty([max_jobs, 2])\n",
    "configs = []\n",
    "for i, t in enumerate(my_tuner.analytics().training_job_summaries()):\n",
    "    jn = t['TrainingJobName']\n",
    "    df = sagemaker.analytics.TrainingJobAnalytics(jn).dataframe()\n",
    "\n",
    "    row = history[history['TrainingJobName'] == jn]\n",
    "    config = {'num_heads': int(row['num_heads']), 'num_layers': int(row['num_layers']), 'num_units': int(row['num_units'])}\n",
    "    configs.append(config)\n",
    "    for j, metric in enumerate(metric_definitions):\n",
    "        metric_name = metric['Name']\n",
    "        data[i, j] = float(df[df['metric_name'] == metric_name]['value'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_params_original_model = sum(\n",
    "            p.numel() for p in model.parameters() if p.requires_grad\n",
    "        )\n",
    "plt.axvline(n_params_original_model, color='black', linestyle='--')\n",
    "\n",
    "from multi_objective import get_pareto_optimal\n",
    "\n",
    "idx = get_pareto_optimal(data)\n",
    "x = data[idx, 0]\n",
    "y = data[idx, 1]\n",
    "\n",
    "plt.scatter(x, y, marker='o', s=80,\n",
    "            facecolors='none', edgecolors='C0', linewidth=2, label='Pareto front')\n",
    "plt.xlabel('number of parameters')\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.grid(linewidth='1', alpha=0.4, which=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Select Final Model\n",
    "\n",
    "Finally, we can select the model from the Pareto set that gives us the right balance between parameter count and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# architecture_definition = pareto_set['configs'][0]\n",
    "# new_model = get_final_model(original_model=model, architecture_definition=architecture_definition)\n",
    "#\n",
    "# n_params_new_model = sum(\n",
    "#             p.numel() for p in new_model.parameters() if p.requires_grad\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# batch = train_dataset[0][sentence1_key]\n",
    "# latency = compute_latency(model, tokenizer, batch, device)\n",
    "# print(f'Latency old model = {latency}')\n",
    "#\n",
    "# latency = compute_latency(new_model, tokenizer, batch, device)\n",
    "# print(f'Latency old model = {latency}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}