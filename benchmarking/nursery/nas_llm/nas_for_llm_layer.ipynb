{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from training import train_supernetwork\n",
    "from search import multi_objective_search\n",
    "from parse_model import get_final_model\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "task_name = 'rte'\n",
    "model_type = 'bert-base-cased'\n",
    "output_dir = 'nas_output_dir'\n",
    "max_seq_length = 128\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data loading\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"glue\", task_name\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "if model_type.startswith(\"gpt2\"):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing the raw_datasets\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(\n",
    "        *args, padding=padding, max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    # if label_to_id is not None and \"label\" in examples:\n",
    "    #     result[\"label\"] = [\n",
    "    #         (label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]\n",
    "    #     ]\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split data in training / validation / test\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\n",
    "    \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n",
    "]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"idx\"])\n",
    "test_dataset = test_dataset.remove_columns([\"idx\"])\n",
    "\n",
    "# Split training dataset in training / validation\n",
    "split = train_dataset.train_test_split(\n",
    "    train_size=0.7, seed=0\n",
    ")  # fix seed, all trials have the same data split\n",
    "valid_dataset = split[\"test\"]\n",
    "\n",
    "if task_name in [\"sst2\", \"qqp\", \"qnli\", \"mnli\"]:\n",
    "    valid_dataset = Subset(\n",
    "        valid_dataset,\n",
    "        np.random.choice(len(valid_dataset), 2048).tolist(),\n",
    "    )\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_type,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_type,\n",
    "    config=config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train weight-sharing based super-network\n",
    "training_args = TrainingArguments(output_dir=output_dir)\n",
    "training_args.search_space = 'small'\n",
    "training_args.use_accelerate = False # set this to True to distribute training on multiple GPUs\n",
    "training_args.is_regression = False  # set this to True if your dataset is a regression dataset, for example STSB\n",
    "training_args.save_strategy = \"epoch\"\n",
    "training_args.log_dir = '.log_dir'\n",
    "\n",
    "train_supernetwork(model, train_dataloader, eval_dataloader, metric, training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multi-objective search for sub-networks\n",
    "metric_name = 'accuracy'\n",
    "training_args.num_samples = 5\n",
    "pareto_set = multi_objective_search(model, eval_dataloader, metric, metric_name, training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Pareto front\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_params_original_model = sum(\n",
    "            p.numel() for p in model.parameters() if p.requires_grad\n",
    "        )\n",
    "plt.axvline(n_params_original_model, color='black', linestyle='--')\n",
    "\n",
    "plt.scatter(pareto_set['params'], pareto_set['error'], marker='o', s=80,\n",
    "            facecolors='none', edgecolors='C0', linewidth=2, label='Pareto front')\n",
    "plt.xlabel('number of parameters')\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.grid(linewidth='1', alpha=0.4, which=\"both\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate final model\n",
    "# Instantiate final model\n",
    "print(pareto_set['params'][0], pareto_set['error'][0])\n",
    "architecture_definition = pareto_set['configs'][0]\n",
    "print(architecture_definition)\n",
    "new_model = get_final_model(original_model=model, architecture_definition=architecture_definition)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "n_params_new_model = sum(\n",
    "            p.numel() for p in new_model.parameters() if p.requires_grad\n",
    "        )\n",
    "print(n_params_new_model)\n",
    "\n",
    "import torch\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs\\\n",
    "        = new_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "print(eval_metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}