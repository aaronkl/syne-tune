{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "from training import train_supernetwork\n",
    "from search import multi_objective_search\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "task_name = 'rte'\n",
    "model_type = 'bert-base-cased'\n",
    "output_dir = 'nas_output_dir'\n",
    "\n",
    "#TODO: fix\n",
    "max_seq_length = 128\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    \"glue\", task_name\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"glue\", task_name)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "if model_type.startswith(\"gpt2\"):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing the raw_datasets\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "\n",
    "# Padding strategy\n",
    "padding = \"max_length\"\n",
    "\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(\n",
    "        *args, padding=padding, max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    # if label_to_id is not None and \"label\" in examples:\n",
    "    #     result[\"label\"] = [\n",
    "    #         (label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]\n",
    "    #     ]\n",
    "    return result\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "test_dataset = raw_datasets[\n",
    "    \"validation_matched\" if task_name == \"mnli\" else \"validation\"\n",
    "]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"idx\"])\n",
    "test_dataset = test_dataset.remove_columns([\"idx\"])\n",
    "\n",
    "# Split training dataset in training / validation\n",
    "split = train_dataset.train_test_split(\n",
    "    train_size=0.7, seed=0\n",
    ")  # fix seed, all trials have the same data split\n",
    "valid_dataset = split[\"test\"]\n",
    "\n",
    "if task_name in [\"sst2\", \"qqp\", \"qnli\", \"mnli\"]:\n",
    "    valid_dataset = Subset(\n",
    "        valid_dataset,\n",
    "        np.random.choice(len(valid_dataset), 2048).tolist(),\n",
    "    )\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=per_device_eval_batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_type,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=task_name,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_type,\n",
    "    config=config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=output_dir)\n",
    "training_args.search_space = 'small'\n",
    "training_args.use_accelerate = False # set this to True to distribute training on multiple GPUs\n",
    "training_args.is_regression = False  # set this to True if your dataset is a regression dataset, for example STSB\n",
    "training_args.save_strategy = \"epoch\"\n",
    "training_args.log_dir = '.log_dir'\n",
    "\n",
    "train_supernetwork(model, train_dataloader, eval_dataloader, metric, training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_name = 'accuracy'\n",
    "training_args.num_samples = 5\n",
    "pareto_set = multi_objective_search(model, eval_dataloader, metric, metric_name, training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pareto_set['params'], pareto_set['error'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = get_final_model(pareto_set.masks[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}